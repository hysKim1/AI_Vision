{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [AI 이노베이션 스퀘어] 12기 고급 시각반\n",
    "- github: https://github.com/hysKim1/AI_Vision\n",
    "    - [파이썬 공식문서]https://docs.python.org/ko/3/\n",
    "    - [Numpy 공식문서]https://numpy.org/devdocs/\n",
    "    - [ML]https://developers.google.com/machine-learning/crash-course/ml-intro?hl=ko\n",
    "    - [DL]https://www.tensorflow.org/?hl=ko\n",
    "    \n",
    "    \n",
    "---\n",
    "* [Numpy,Scipy,Scikit-image, PIL를 통한 기본 영상 처리5](#Numpy,Scipy,Scikit-image,-PIL를-통한-기본-영상-처리5)\n",
    "    * [einsum](#einsum)\n",
    "      * [Vector Multiplication](#Vector-Multiplication)\n",
    "      * [Matrix](#Matrix)\n",
    "          * [Diagonal](#Diagonal)\n",
    "          * [Trace](#Trace)\n",
    "      * [Vector, Matrix Multiplication](#Vector,-Matrix-Multiplication)\n",
    "      * [ Matrix Multiplication](#Matrix-Multiplication)\n",
    "          * [Batch Matrix Multiplication](#Batch-Matrix-Multiplication)\n",
    "      * [Ellipsis](#Ellipsis)\n",
    "* [OPENCV를 활용한 영상 처리1](#OPENCV를-활용한-영상-처리1)\n",
    "  * [OpenCV](#OpenCV)\n",
    "  * [ OpenCV 머신러닝 맛보기](#OpenCV-머신러닝-맛보기)\n",
    "\n",
    "\n",
    "## 영상처리를 위한 딥러닝1\n",
    "### computer vision\n",
    "- 딥러닝 기반 기법 배움\n",
    "- 컴퓨터 비전 인간의 비전 시스템을 흉내냄\n",
    "    - 눈 - 뇌 - 이미지 인식\n",
    "    - sensing devices - computer(**Interpreting Devices**) -> output \n",
    "    computer vision \n",
    "    - 패러다임: DL 전제 조건이 많음\n",
    "    - 전제조건: 돈이 많아서 많은 data, processing machine 충분\n",
    "        - 영상에서는 머신러닝, 딥러닝을 사용하지 않는 방법도 알아야함\n",
    "            - 기술 부채(techinical debt) : 딥러닝으로 성공한 회사 얼마 없음\n",
    "              - Google Brain, Deep mind : 대부분의 인공지능회사에서는 투자에 비해서 엄청난 적자 \n",
    "          인식 삼총사\n",
    "          1. 이미지 분류 classificaiton\n",
    "          2. object detection : localization + classificaiton \n",
    "              - 이미지 영역에 무엇이 있고 어디에 무슨 애인지\n",
    "          3. segmentation - 픽셀단위로 인식\n",
    "          \n",
    "          문제 도메인에 따라서 labelling 된 train data가 달라짐\n",
    "          \n",
    "              \n",
    "### 과정\n",
    "1. Input Data\n",
    "    - image, video\n",
    "    - matrix -> vector 형태로 표현    \n",
    "    - 전통적인 ml에서 이미지만 feature 관련된 부분이 필수\n",
    "            - \n",
    "2. Pre-processing\n",
    "    - 크기 맞춤 \n",
    "3. ***Feature extraction\n",
    "    - 학습을 통해서 특별한 feature  추출\n",
    "    - 영상처리에서의 핵심, 필수 \n",
    "        - image 자체는 pixel당 계산을 해야되서 연산량 복잡\n",
    "            - 전체이미지 사용보다 feature extraction기반으로 변형된 데이터 사용\n",
    "            - task 머신러닝으로하는게 한정ㅈ 목적에 필요없는 feature는 있을 필요가 없음\n",
    "            - feature extraction 통해서 연산량 줄임   \n",
    "    - 학습데이터를 통해서 다른 클래스와 분류되고 그 클래스의 공통된 feature vector 추출\n",
    "        - 예. 오토바이는 두개의 동글나 바퀴, 손잡이가 있음\n",
    "\n",
    "4. ML Model\n",
    "    - 이미지가 새로운 이미지가 들어왔을때 featur vector MLE 통해서 확률 가장 큰것으로 분류함\n",
    "\n",
    "### Imag as Function\n",
    "- 이미지는 함수(function)\n",
    "    - 2차원 이미지의 좌표값 x,y가 파라미터인 함수\n",
    "        - x,y값 따라서 해당 좌쵸값의 pixel에 매핑\n",
    "            - 예. 흑백이미지 F( x,y) =255 (White Pixel) \n",
    "            - 예.컬러이미지 F(0,0)=[ R,G,B 픽셀값]\n",
    "            - 이미지의 metatdata확인해서 channel 순서 확인 필요함 \n",
    "       -  예.곤충 벌의 겹눈(compound eyes) 좌표계 측면으로 사물 인식\n",
    "        - 컴퓨터는 이미지를 단순히 수열의 matrix로만 보며 패턴익식으로 파악\n",
    "            \n",
    "   - 이미지 함수로 표현 가능\n",
    "       - 이미지에대한 합성함수로 표현 가능\n",
    "           - 어던 값이 들어 왔을때 학률기반으로 표현도 가능 \n",
    "           \n",
    "### No Free Lunch Theorm(NFL)\n",
    "1. Simplication of reality : 모델은 현실세계를 다 반영할 수 없음 \n",
    "2.  수집의 한계성으로 현실을 다 반영 할 수 없어서 축소판이라는 가정 -> Model Bias 생김, 특별한 경우에 잘됨\n",
    "  > \"No one model work best for all possible situtations.\"\n",
    "      > The supervised learning no-free-lunch theormes(2002)\n",
    "      > No Free Lunch Theormes for Optimizations(1997)\n",
    "  - **optimization** :ML에서는 loss function  최소화하는 파라미터 찾음\n",
    "  - 특정 모델이 특정 상황에서 최적화되기 때문에 모델 Selection 중요함 \n",
    "  - feature extraction: 항상 잘되는 feature 없다\n",
    "  - CNN 컨볼루션 : 연산된 결과가 feature , feature 로 표현한 것을 feature map\n",
    "      - 최적화 관점에서 제일 좋은 feature 찾음\n",
    "      - 항상 좋은 방법은 없음 \n",
    "      - 방법을 많이 알수록 좋음\n",
    "   - 머신러닝의 가장 핵심 내용\n",
    "   - Kaggle : No Free Hunch \n",
    " - DL : 데이터에 따라서 layer, node 갯수를 조정해야함 \n",
    " - 머신러닝에서는 데이터 기반 \n",
    "- 논리학: 귀납법- 각 각의 데이터라는 사실을 통해 일반화된 법칙을 뽑음\n",
    "    - black swan, 즉 outlier 어느정도는 인정해줌 \n",
    "\n",
    "> The Master Algorithm by Pedro Dmoingos\n",
    "    - 빌게이츠가 읽은 인공지능 관련 책 2권, 시진핑 서가에 있음 \n",
    "    >  A Few Userful Things to know about Machine Learning\n",
    "        - NFL 과 반대 되는 개념\n",
    "        - 모든 상황에서 잘 되는 알고리즘\n",
    "        - 아직 못 만들었기 때문에 특별한 상황에 \n",
    "        \n",
    "### Image Preprocessing\n",
    "- 이미지에 미리 처리해야함\n",
    "#### 필수 전처리\n",
    "1. 이미지 사이즈 조정\n",
    "2. 이미지 색상 정보\n",
    "\n",
    "#### 성능 향상을 위한 전처리\n",
    "1. Normalization\n",
    "2. Dimensionality Reduction\n",
    "    - 컬러 이미지(3차원)을 흑백화(2차원)\n",
    "        - 의외로 컬러 색상정보는 많이 필요하지 않고 texture, 형태 이용\n",
    "        - 필요한 경우: 자율 주행 자동차에서 도로에서는 색상이 중요함( 랑,흰색 ) 대부분은 \n",
    "\n",
    "    - complexity, computation overhead 줄임\n",
    "3. OpenCV ,PIL ,통해서 Data Augmentation\n",
    "    - 이미지 증강\n",
    "    1. de-texturiezed\n",
    "    2. De-colorized\n",
    "    3. edge enhanced\n",
    "    4. Salient edge map\n",
    "    5. flip, rotate\n",
    "        - 성능 향상보다는 과적합 방지용으로 사용함\n",
    "\n",
    "### features\n",
    "- ML에서 배운 feature과 유사함 (다른점 존재 \n",
    "- 이미지에서는 local, global feature 개념이 존재 \n",
    "    - global feature : 전체이미지에대한\n",
    "    - local feature: 특정 영역에대한 feature\n",
    "- cost, target 정하는데에 사용\n",
    "- ML 모델 1개는 1가지만 가능\n",
    "    - 약 인공지능:\n",
    "        - 현재의 머신러닝,인공지능\n",
    "        - Yann LeCun 시사 in : https://www.sisain.co.kr/news/articleView.html?idxno=32487\n",
    "   - 강 인공지능:\n",
    "- feature : 이미지 관점에서는 **이미지를 구분하는 유일한 특성**\n",
    "    - domain 마다 용어가 다름\n",
    "    - ML:feature, dimension, \n",
    "        > The Unreasonble effectiveness of Data(2009) by Peter Norvig\n",
    "            - 데이터가 많을수록 데이터가 성능이 항상 좋은게 아님\n",
    "                - DL: 데이터가 많을 수록 성능이 좋음\n",
    "                - ML: 어느 특정 수의 데이터가 있으면 성능 향상이 크게 늘어나지않음\n",
    "            - 전처리 중요함\n",
    "            - Learning curves for Confusion set disambigatoin (2001) by Michele Banko & Eric Brill\n",
    "                  - 알고리즘 개선하려해도  ML은 알고리즘 성능이 큰 차이가 없었음 \n",
    "     \n",
    "- 다른 특징이 아닌 1가지 일에만 집중할수 있음\n",
    "  - 예. 고양이개 분류할때 다른 동물 특징 뽑을 필요 없음\n",
    "           \n",
    "          \n",
    "#### Featured Data\n",
    "- Raw 데이터 가공한 데이터\n",
    "- 있는거 전처리 잘 하면 없는것보다 어느정도의 성능 확보 가능\n",
    "- computation 줄임, 메모리 효율적\n",
    "- 가공 기술마다 성능이 핵심\n",
    "    > kaggle 대회- 90% 전처리 (데이터에대한 이해 중요)\n",
    "- 현실에서는 noise가 너무 많아 targeting 못하기 때문에 가공한 데이터가 좋음\n",
    "    - 가공에서도 정보의 손실(information loss)발생으로 가정이 생김\n",
    "    - 이미지 여러장을 통해서 feature 추출 (eg.one-shot learning)\n",
    "    - 데이터가 적은경우 성능을 높이는 방법론: data augmentation,  transfer learning, one-shot learning, zero-shot learning , 강화학습...q\n",
    "     - 고유의 특징이 반복성,일관성이 있어야함\n",
    "     \n",
    "Image Feature는 ML의 Featured data와 동일함\n",
    "- local features\n",
    "    -  edge, keypoints\n",
    "- global features\n",
    "    - color histogram ,pixel count\n",
    "\n",
    "### 좋은 특징\n",
    "-  Idnetifiable (구분가능)\n",
    "- easily tracked, compared\n",
    "- 오캄의 면도날(같다면 복잡한거 보다는 간단한게 좋다)\n",
    "- 불변성(상태에 따라 변화가 없어야함)\n",
    "    - scale, lighting sondition, viewsing scale\n",
    "- noise 가 있어도 구분이 가야함\n",
    "\n",
    "- 영상에서의ML -> pattern 찾기 주요\n",
    "    - Histogram of Oriented Gradients (HOG)\n",
    "    - Haar Cascades\n",
    "    - Scale-Invariant Feature Transfor(sift)\n",
    "  \n",
    "    - CNN: **특징 찾는것을 학습을 통해서** 찾음 \n",
    "         - local : 특징 찾기에서는  전체적 특징이 아닌 각각의 특징을 찾아야함 \n",
    "         - Feature Crosses(교차 특성) \n",
    "             - 머신러닝 단기집중과정 머신러닝 용어집 :특성교차(feature cross)\n",
    "             - ml playground:https://playground.tensorflow.org/\n",
    "                 - 각각의 특징 들이 곱해져서 새로운 특징으로 ㅂㅕㄴ\n",
    "                 - 동시에 2개 이상써서 복잡하게 사용 \n",
    "            \n",
    "                  - convolution vs correlation\n",
    "                      - convolution flip 왜 했는지\n",
    "                          1. linear combination 안하기 때문에 특성대로 안나오고 분할됨\n",
    "                              - 주파수 영역 자기 보전 \n",
    "                              - convolution : flip 통해서 영역간의 변화시 값 보전(선형결합 유지)  커널통해 변화된 결과를 알기 위해서\n",
    "                              -  kernel 통해서 새롭게 표현된 특성 \n",
    "                                  - 영역 이동시켜서 선형 결합으로 유지\n",
    "                                  - 컨볼루션 연산 결과에서 특성 반영 여부를 알기위해서\n",
    "                                   \n",
    "      - correlation : 유사도 측정\n",
    "          - 딥러닝에서 하는 연산\n",
    "          - 학습된 커널/필터 찾는게 목적\n",
    "          - 공간 변형, 이동하는 관점: kernel 통해 특성을 만듦\n",
    "          - 원본모양유지 (flip)                     \n",
    "     - convolution 연산 -  특정 kernel에 따라서 변형된 결과\n",
    "     - kernel 잘 바꾸면 서로의 **특징들을 학습**을 통해서 찾음\n",
    "         - 예. 1 세로 1개, 0 타원형 1개\n",
    "         - 예. 강아지, 고양이 특징은 복잡함\n",
    "         - CNN : 특징 합습시켜서 새로운 형태의 데이터로 분류 - 2차원의 공간 정보를 갖음\n",
    "             - translational Invariance 변형의 불변성\n",
    "             - Locality : filter 크기만큼의 이미지의 부분을 봄 \n",
    "                 - 특징 학습이 잘 될거라 생각 못함..\n",
    "                 - feature extraction + model 분리하지만 CNN 합쳐서함\n",
    "                 - R-CNN : end-to-end 안되어서 SVM사용했었음\n",
    "                 - 연산량은 복잡하지만 전체 이미지 사용보다는 연산량 덜 복잡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_pca_faces() missing 3 required positional arguments: 'X_train', 'X_test', and 'image_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2df34de4ce32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmglearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_pca_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: plot_pca_faces() missing 3 required positional arguments: 'X_train', 'X_test', and 'image_shape'"
     ]
    }
   ],
   "source": [
    "mglearn.plots.plot_pca_faces()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "얼굴에 복잡한애들을 차원축소하여 얼굴인식 \n",
    "이전에는 이런방식으로 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "iris= sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species\n",
       "0             5.1          3.5           1.4          0.2     setosa\n",
       "1             4.9          3.0           1.4          0.2     setosa\n",
       "2             4.7          3.2           1.3          0.2     setosa\n",
       "3             4.6          3.1           1.5          0.2     setosa\n",
       "4             5.0          3.6           1.4          0.2     setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  virginica\n",
       "146           6.3          2.5           5.0          1.9  virginica\n",
       "147           6.5          3.0           5.2          2.0  virginica\n",
       "148           6.2          3.4           5.4          2.3  virginica\n",
       "149           5.9          3.0           5.1          1.8  virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris #하나의 행에 하나씩 들어감 4차원 데이터 전통적인 데이터는 1차원으로 받음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length       5.1\n",
       "sepal_width        3.5\n",
       "petal_length       1.4\n",
       "petal_width        0.2\n",
       "species         setosa\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### why Deep Learning\n",
    "1. State of Art(SOTA)\n",
    "2. feature extraction 어려운데 feature extraction + Model연동 \n",
    " \n",
    " \n",
    " - 오토바이 이미지  feature extraction 해서 feature vector화\n",
    " - 전통적인 ML은 1차원으  입력 받아서 형태를 맞추기 위해서 flatten - 데이터를 \n",
    "     - **feature extraction + classifie**r의 가교(bridge) 역할 \n",
    "     - DL은 이 두가지를 하나의 모델로 만듦\n",
    "     - 학습을 통해서 feature를 찾음\n",
    "     - 딥하게 만들면 앞에 애들은 feature extraction 으로 작용 \n",
    "     - 끝의 레이어는 분류 레이어 \n",
    "     - > Colah  blog ; maniforlds, topology공간을 왜곡 시켜서 이미지 관점에서 \n",
    "     - 이미지의  특성을 갖는 feature형태로 바꿈  \n",
    "     - 입력레이어 외에는 flatten 할 필요 없이 하나의 모델에서 함\n",
    "     - CNN 중간에 Flatten  집어넣는 이유는 ML\n",
    "         -  왜 딥러닝이 머신러닝보다 선호되는지\n",
    "         - 머신러닝의 고원현상: 데이터가 많아도 더 이상 올라가지 않는 구간이 있음\n",
    "         -  딥러닝: 공간의 왜곡하는 관점 : 비선형 활성화 함수 때문에 최종 결과를 위해서 계속 왜곡하기 때문에 data space가 선형 분류되도록 나눠짐 \n",
    "         - 레이어 깊을수록 성능이 좋아짐 \n",
    "             - Data, Computation resources \n",
    "             - 32pg 복잡한 모델은 안거쳐도 됨\n",
    "               automatic feature extraction\n",
    "         \n",
    "     A. End-to -End Models\n",
    "         - Feature Extractor , Classifier 역할들 한번에 함\n",
    "         - 전통적인 방법을 바꾸는 paradig\n",
    "     B. 모델 재용 \n",
    "     C. 좋은 성능\n",
    "     D. 일반화\n",
    "     \n",
    "#### dense\n",
    "- 전통적인 fully connected model\n",
    "- layer, node갯수는 생성자가 마음대로 지정\n",
    "- output layer는 모델에 따라 다름(\n",
    "    - Classification : 클래스 갯수 만큼\n",
    "    - Regression 1개\n",
    "- input **1차원** 형태로 입력 받음 \n",
    "  - `tf.keras.layers.Flatten(input_shape=(28, 28))`\n",
    "\n",
    "\n",
    " 참고 :https://www.tensorflow.org/tutorials/quickstart/beginner?hl=ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 힙성곱 신경망 \n",
    "\n",
    "Con2D \n",
    "1. Feature extraction\n",
    "\n",
    "\n",
    "1ckdnjs \n",
    "\n",
    "inup -> feature extractor -> feature vector(flattened) -> traditional ML algoritm -> output\n",
    "\n",
    "feature extraction이 잘되면 ( 좋은 데이터 사용하면) 구분이 쉽기 때문에 모델이 간단함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extractor: 합성곱 + pooling\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifiter 추론과정\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능이 안 좋은 경우\n",
    "1. 데이터 수집\n",
    "2. 모델 변경\n",
    "    - 머신러닝부터 확인\n",
    "    - 딥러닝은 데이터가 많아야 성능이 좋아지기 때문에 머신러닝이 더 좋은 구간이 존재함\n",
    "        > 책:머신러닝 실무 프로젝트 \n",
    "                - **머신러닝을 사용하지 않는 방법 검토하기** \n",
    "                - **딥러닝 사용하지 않는 방법 검토하기** \n",
    "                -  Machine Learning :The high-interests credit card of technical debt\n",
    "                     - 머신러닝은 기술부채 중 이자율이 매우 높은 신용카드다\n",
    "                     - 머신러닝을 사용해서 성공 못할 확률이 높음 (사업성이 없음)\n",
    "\n",
    "\n",
    "- 대안 : transfer learning 에서 적은 비용 듦\n",
    "\n",
    "> Succedding with AI\n",
    "    - 비즈니스에서 AI 적용\n",
    "    - 경제성의 원리: 인건비가 비싼 국가에서는 인공지능에 막대한 투자함\n",
    "\n",
    "#### Application\n",
    "1. Classification\n",
    "    - CNN +Trsnfer learning 으로 간단해짐\n",
    "    - 이미지를 주면 무엇인지 구분\n",
    "2. Identification\n",
    "    - 같은 객체별 구분\n",
    "    - 예. 사람들 중에서 누구인지 구분(face)\n",
    "3. localiation\n",
    "     - 어디에 있는지\n",
    "4. detection\n",
    "     - localization + classification\n",
    "     \n",
    "5. object detection\n",
    "     - RCNN, Fast RCNN, YOLO,SDD\n",
    "6. instance segmentation\n",
    "    - pixel 단위\n",
    "    - FCN, U-net, M-RCNN\n",
    "7. pose detection\n",
    "8. Image Caption\n",
    "    - 사진 설명 규칙을 template에 따라서 설명\n",
    "    - RNN (비디오 처리)\n",
    "9. style transfer \n",
    "    -  생성모델\n",
    "10. GAN\n",
    "    - 이미지 생성\n",
    "    \n",
    "    \n",
    "Classification\n",
    "-  컨볼루션 신경망, 이미지 분류, TF Hub 이용한 전이학습, 컨볼루션 신경망\n",
    "segmentation\n",
    "- 분절화 \n",
    "생성\n",
    "- Stype Transfer, DCGAN, AutoEncoder, U-net,FCN\n",
    "이미지 캡션\n",
    "\n",
    "공식문서 이미지\n",
    "- tf, pytorch 거의 비슷함\n",
    "- 영상처리는torch vision, 자연어처리 pytorch가 간명, 영상처리는 pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #GPU 필요\n",
    "tf.debugging.set_log_device_placement(True) #GPU 설정이 되어있는"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.constant([1,2,3])\n",
    "a+a # CPU/GPU 인지 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL 프레임 워크\n",
    "### Theano \n",
    "- 최초로 성공한 딥러닝 프레임워크\n",
    "- 이론과 방법론간의 차이가 있어서 이론처럼 코딩하도록 쉽게 사용하도록 나옴\n",
    " - 이론처럼 코드 작성하면 theano로 실행되도록 나옴\n",
    "     - keras가 그 중하나\n",
    "         - backend 내부적으로는 티아노를 사용하고 코드는 functional paradigm으로 keras\n",
    "\n",
    "### Pytorch\n",
    "- keras, tf 사이\n",
    "- 더 low level \n",
    "- prototype 많들때 좋음(시스템화 불필요)\n",
    "\n",
    "- 버전 올라갈수록 서로 배낌\n",
    "\n",
    "### TensorFlow \n",
    " - keras 문법 간결하고 이론과 유사하여 tf를 쉽게 사용가능\n",
    " - MS 의 CNTK까지 지원 \n",
    " - 과감하게 tf1포기하고 keras전면적으로 내세우고 결과값이 바로 나오는 형태로 바꿈\n",
    " - 아직 기능이 많아서 막강함\n",
    " \n",
    "- keras 가져다 쓰면 내부적으로 tf쓴거와 동일함\n",
    "- 복잡한 모델을 tf로 배울 필요는 없음\n",
    "- 프레임 워크에대한 이해를 넓히기 위해 tensor 이해 \n",
    "- tensor는 만드는 방법이 5가지 \n",
    "    - 혼합 사용 가능 \n",
    "        1. keras Sequential\n",
    "        2. kerasmodel\n",
    "        3. keras subclass model\n",
    "          -  tf2에서 서로 거의 연동되지만 tf1과는 호환이 안되기 때문에 \n",
    "        4. tensor    tf.nn pytorch에서 차용\n",
    "        5. estimator scikit-learn에서 차용\n",
    "        \n",
    "- 영상처리 부분만 tf이용 \n",
    "- 2.1기준  최소 2.2 이상 예시가 있어서..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
